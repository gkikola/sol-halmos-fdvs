\sectionnum{7}
\section{Linear Dependence, Linear Combinations, and Bases}

\Exercise1
\begin{enumerate}
\item Prove that the four vectors
  \begin{align*}
    x &= (1, 0, 0), \\
    y &= (0, 1, 0), \\
    z &= (0, 0, 1), \\
    u &= (1, 1, 1),
  \end{align*}
  in $\C^3$ form a linearly dependent set, but any three of them are
  linearly independent.
  \begin{proof}
    The vectors $x$, $y$, $z$, and $u$ are linearly dependent since
    \begin{equation*}
      x + y + z - u = 0.
    \end{equation*}
    However any three them form a linearly independent set, as we will
    now show. It should be clear that the set $\{x,y,z\}$ is linearly
    independent, so we just need to show that any two of $x,y,z$ taken
    together with $u$ forms a linearly independent set.

    There are three cases, but we can handle them all at once by
    observing the symmetry in the coordinates of $x$, $y$, and
    $z$. If, for example, $x$, $y$, and $u$ are independent, then
    exactly the same reasoning and indeed the same system of equations
    (with some variable indices switched around) will show that $x$,
    $z$, and $u$ are independent and that $y$, $z$, and $u$ are
    also. Therefore we will only show the linear independence of $x$,
    $y$, and $u$, and that will suffice.

    So let
    \begin{equation*}
      \alpha_1x + \alpha_2y + \alpha_3u = 0.
    \end{equation*}
    By equating the third coordinate of each side we get
    $\alpha_3 = 0$. Then from the first coordinates we get
    $\alpha_1 + \alpha_3 = 0$ so that $\alpha_1 = 0$. Similarly, from
    the second coordinates we get $\alpha_2 = 0$. Therefore
    $\alpha_1,\alpha_2,\alpha_3$ must all be zero, which establishes
    the linear independence of the three vectors, completing the
    proof.
  \end{proof}
\item If the vectors $x$, $y$, $z$, and $u$ in $\P$ are defined by
  $x(t) = 1$, $y(t) = t$, $z(t) = t^2$, and $u(t) = 1 + t + t^2$,
  prove that $x$, $y$, $z$, and $u$ are linearly dependent, but any
  three of them are linearly independent.
  \begin{proof}
    Observe that there is a natural correspondence between these
    vectors and the vectors in the previous part of this exercise:
    each of $x$, $y$, and $z$ have one coefficient equal to $1$ and
    the others $0$, while $u$ has all coefficients equal to $1$. So
    the same relation
    \begin{equation*}
      x + y + z - u = 0
    \end{equation*}
    shows that the four vectors are linearly dependent, while exactly
    the same reasoning as before will show that any three of them are
    linearly independent.
  \end{proof}
\end{enumerate}

\Exercise2 Prove that if $\R$ is considered as a rational vector
space, then a necessary and sufficient condition that the vectors $1$
and $\xi$ in $\R$ be linearly independent is that the real number
$\xi$ be irrational.
\begin{proof}
  First suppose $\xi$ is irrational. If possible, let there be scalars
  $\alpha_1$ and $\alpha_2$, not both zero, such that
  \begin{equation*}
    \alpha_1 + \alpha_2\xi = 0.
  \end{equation*}
  Then if $\alpha_2$ is nonzero we can solve for $\xi$ to get
  \begin{equation*}
    \xi = -\frac{\alpha_1}{\alpha_2},
  \end{equation*}
  which is rational, a contradiction. On the other hand if $\alpha_2$
  is zero then $\alpha_1$ must be zero as well, again a
  contradiction. So $1$ and $\xi$ must be linearly independent.

  Conversely, suppose that $1$ and $\xi$ are linearly independent. If
  $\xi$ is rational, then we can choose rational scalars
  $\alpha_1 = -\xi$ and $\alpha_2 = 1$ so that
  $\alpha_1 + \alpha_2\xi = 0$, contradicting the assumption of linear
  independence. So $\xi$ must be irrational.
\end{proof}

\Exercise3 Is it true that if $x$, $y$, and $z$ are linearly
independent vectors, then so also are $x + y$, $y + z$, and $z + x$?
\begin{solution}
  No, this is not always true, depending on the nature of the field of
  scalars, as we will soon see. Let $x$, $y$, and $z$ be linearly
  independent vectors in a vector space.

  Now suppose that
  \begin{equation*}
    \alpha_1(x + y) + \alpha_2(y + z) + \alpha_3(z + x) = 0,
  \end{equation*}
  for scalars $\alpha_1,\alpha_2,\alpha_3$. Then
  \begin{equation*}
    (\alpha_1 + \alpha_3)x + (\alpha_1 + \alpha_2)y
    + (\alpha_2 + \alpha_3)z = 0.
  \end{equation*}
  Now, since $x$, $y$, and $z$ are linearly independent, each of the
  above coefficients must be zero. So $\alpha_1 = -\alpha_3$,
  $\alpha_1 = -\alpha_2$ and $\alpha_3 = -\alpha_2$. But then we have
  \begin{equation*}
    \alpha_2 = -\alpha_1 = \alpha_3 = -\alpha_2.
  \end{equation*}
  Now, if our vector space is over the field $\R$ or $\C$, then
  $\alpha_2 = -\alpha_2$ implies that $\alpha_2 = 0$ so that
  $\alpha_1$ and $\alpha_3$ are also zero. This would show linear
  independence.

  However, consider what happens in the field $\Z_2$! We saw this and
  other finite fields in Exercise~\ref{exercise:the-field-Zp}. In
  $\Z_2$, we have $1 + 1 = 0$ or $1 = -1$. So if our vector space is
  over this field, then we may let
  $\alpha_1 = \alpha_2 = \alpha_3 = 1$. Then
  \begin{multline*}
    (x + y) + (y + z) + (z + x) = (1 + 1)x + (1 + 1)y + (1 + 1)z \\
    = 0x + 0y + 0z = 0,
  \end{multline*}
  so that these vectors are in fact linearly dependent even if $x$,
  $y$, and $z$ are independent.
\end{solution}

\Exercise4
\begin{enumerate}
\item Under what conditions on the scalar $\xi$ are the vectors
  $(1+\xi, 1-\xi)$ and $(1-\xi, 1+\xi)$ in $\C^2$ linearly dependent?
  \begin{solution}
    Let $x = (1+\xi,1-\xi)$ and $y = (1-\xi, 1+\xi)$. If $x$ and $y$
    are linearly dependent, then there are $\alpha_1$ and $\alpha_2$
    in $\C$, not both zero, for which $\alpha_1x + \alpha_2y =
    0$. This gives
    \begin{align}
      \alpha_1(1 + \xi) + \alpha_2(1 - \xi) &= 0
      \label{eq:x-y-lin-dep-eq-1}\\
      \alpha_1(1 - \xi) + \alpha_2(1 + \xi) &= 0.
      \label{eq:x-y-lin-dep-eq-2}
    \end{align}
    Subtracting \eqref{eq:x-y-lin-dep-eq-2} from
    \eqref{eq:x-y-lin-dep-eq-1} then gives
    \begin{equation*}
      2\alpha_1\xi - 2\alpha_2\xi = 0.
    \end{equation*}
    So either $\xi = 0$ or $\alpha_1 - \alpha_2 = 0$. But if the
    latter is true, then \eqref{eq:x-y-lin-dep-eq-1} implies that
    $\alpha_1 = \alpha_2 = 0$, which is not possible. So $x$ and $y$
    are linearly dependent if and only if $\xi = 0$.
  \end{solution}
\item Under what conditions on the scalar $\xi$ are the vectors
  $(\xi, 1, 0)$, $(1, \xi, 1)$, and $(0, 1, \xi)$ in $\R^3$ linearly
  dependent?
  \begin{solution}
    Call the vectors $x$, $y$, and $z$, respectively. First note that
    if $\xi = 0$, then $x = z$ and the vectors are obviously linearly
    dependent. So we will assume $\xi\neq0$.

    Now, assume that $x$, $y$, and $z$ are linearly dependent, so that
    there are $\alpha_1,\alpha_2,\alpha_3$ not all zero such that
    \begin{equation*}
      \alpha_1x + \alpha_2y + \alpha_3z = 0.
    \end{equation*}
    Then by equating coordinates we have the following system of
    equations.
    \begin{align*}
      \alpha_1\xi + \alpha_2 &= 0 \\
      \alpha_1 + \alpha_2\xi + \alpha_3 &= 0 \\
      \alpha_2 + \alpha_3\xi &= 0.
    \end{align*}
    Since $\xi$ is nonzero, equating the first and third equations
    gives $\alpha_1 = \alpha_3$. So the second equation becomes
    $2\alpha_1 + \alpha_2\xi = 0$. Then
    \begin{equation*}
      \alpha_2 = -\frac{2\alpha_1}\xi.
    \end{equation*}
    By setting $\alpha = \alpha_1$, we see that if $x$, $y$,
    and $z$ are linearly dependent, then
    \begin{equation}
      \label{eq:dep-rel-for-xi}
      \alpha x - \frac{2\alpha}\xi y + \alpha z = 0.
    \end{equation}
    Equating first coordinates then gives
    \begin{equation*}
      \alpha\xi - \frac{2\alpha}\xi = 0
    \end{equation*}
    or, assuming $\alpha$ is nonzero,
    \begin{equation*}
      \xi^2 = 2.
    \end{equation*}
    It is now easy to verify that when $\xi = \pm\sqrt2$, any nonzero
    choice for $\alpha$ will produce the linear dependence relation
    \eqref{eq:dep-rel-for-xi}.

    To summarize, if either
    \begin{equation*}
      \xi = 0, \quad \xi = \sqrt2, \quad\text{or}\quad \xi = -\sqrt2,
    \end{equation*}
    then $x$, $y$, and $z$ are linearly dependent. Otherwise, they are
    linearly independent.
  \end{solution}
\item What is the answer to (b) for $\Q^3$ (in place of $\R^3$)?
  \begin{solution}
    In this case $\xi$ must be rational. From our previous work, we
    see that the only value of $\xi$ which makes $x$, $y$, and $z$
    linearly dependent is $\xi = 0$.
  \end{solution}
\end{enumerate}

\Exercise5
\begin{enumerate}
\item The vectors $(\xi_1, \xi_2)$ and $(\eta_1, \eta_2)$ in $\C^2$
  are linearly dependent if and only if $\xi_1\eta_2 = \xi_2\eta_1$.
  \begin{solution}
    We will show that this is true. Let
    \begin{equation*}
      x = (\xi_1, \xi_2)
      \quad\text{and}\quad
      y = (\eta_1, \eta_2).
    \end{equation*}
    First note that if either $x$ or $y$ is zero (or if both are
    zero), then the two vectors are obviously linearly dependent and
    the relation $\xi_1\eta_2 = \xi_2\eta_1$ holds. So we may
    henceforth assume that both $x$ and $y$ are nonzero.

    Now, suppose that $x$ and $y$ are linearly dependent. Then there
    are scalars $\alpha_1$ and $\alpha_2$ not both zero such that
    \begin{align}
      \label{eq:xi1-eta1-combination}
      \alpha_1\xi_1 + \alpha_2\eta_1 &= 0 \\
      \label{eq:xi2-eta2-combination}
      \alpha_1\xi_2 + \alpha_2\eta_2 &= 0.
    \end{align}
    Without loss of generality, we will assume that $\xi_1$ is nonzero
    (if $\xi_1 = 0$, then $\xi_2$ must be nonzero and we could proceed
    with a similar argument to the one that follows). From
    \eqref{eq:xi1-eta1-combination} we get
    \begin{equation*}
      \alpha_1 = -\alpha_2\frac{\eta_1}{\xi_1}
    \end{equation*}
    and substituting this for $\alpha_1$ in
    \eqref{eq:xi2-eta2-combination} gives
    \begin{equation*}
      -\alpha_2\frac{\eta_1\xi_2}{\xi_1} + \alpha_2\eta_2 = 0,
    \end{equation*}
    or
    \begin{equation*}
      \alpha_2\left(\eta_2 - \frac{\eta_1\xi_2}{\xi_1}\right) = 0.
    \end{equation*}
    Therefore either $\alpha_2 = 0$ or
    $\xi_1\eta_2 - \eta_1\xi_2 = 0$. Now the first case would imply
    that $\alpha_1 = 0$ also, which cannot be. Therefore we indeed
    have $\xi_1\eta_2 = \xi_2\eta_1$.

    Conversely, suppose $\xi_1\eta_2 = \xi_2\eta_1$. Again, without
    loss of generality we will assume that $\xi_1$ is nonzero. Set
    \begin{equation*}
      \alpha_1 = \eta_1
      \quad\text{and}\quad
      \alpha_2 = -\xi_1.
    \end{equation*}
    Then
    \begin{equation*}
      \alpha_1\xi_1 + \alpha_2\eta_1 = \xi_1\eta_1 - \xi_1\eta_1 = 0
    \end{equation*}
    and
    \begin{equation*}
      \alpha_1\xi_2 + \alpha_2\eta_2 = \xi_2\eta_1 - \xi_1\eta_2 = 0,
    \end{equation*}
    showing that $x$ and $y$ are linearly dependent.
  \end{solution}
\item Find a similar necessary and sufficient condition for the linear
  dependence of two vectors in $\C^3$. Do the same for three vectors
  in $\C^3$.
  \begin{solution}
    Let
    \begin{equation*}
      x = (\xi_1, \xi_2, \xi_3),
      \quad
      y = (\eta_1, \eta_2, \eta_3),
      \quad\text{and}\quad
      z = (\zeta_1, \zeta_2, \zeta_3).
    \end{equation*}
    We note that $x$ and $y$ are linearly dependent if and only if
    \begin{align*}
      \xi_2\eta_3 &= \xi_3\eta_2, \\
      \xi_1\eta_3 &= \xi_3\eta_1, \\
      \intertext{and}
      \xi_1\eta_2 &= \xi_2\eta_1.
    \end{align*}
    The proof is similar to the previous one.

    For the three vectors $x$, $y$, and $z$, a necessary and
    sufficient condition for linear dependence is
    \begin{equation*}
      \xi_1(\eta_2\zeta_3 - \eta_3\zeta_2)
      - \eta_1(\xi_2\zeta_3 - \xi_3\zeta_2)
      + \zeta_1(\xi_2\eta_3 - \xi_3\eta_2) = 0,
    \end{equation*}
    but we will not prove this here.
  \end{solution}
\item Is there a set of three linearly independent vectors in $\C^2$?
  \begin{solution}
    No. Since $\{(1,0), (0,1)\}$ is a basis for $\C^2$, no set of more
    than two vectors in $\C^2$ can be linearly independent. Although
    this fact has not yet been established in the text by Halmos, it
    will be proven in the next section, \S\ 8.
  \end{solution}
\end{enumerate}

\Exercise6
\begin{enumerate}
\item Under what conditions on the scalars $\xi$ and $\eta$ are the
  vectors $(1,\xi)$ and $(1,\eta)$ in $\C^2$ linearly dependent?
  \begin{solution}
    From the previous exercise, we have that $(1,\xi)$ and $(1,\eta)$
    are linearly dependent if and only if $\xi = \eta$.
  \end{solution}
\item Under what conditions on the scalars $\xi$, $\eta$, and $\zeta$
  are the vectors $(1,\xi,\xi^2)$, $(1,\eta,\eta^2)$, and
  $(1,\zeta,\zeta^2)$ in $\C^3$ linearly dependent?
  \begin{solution}
    Again making use of the previous exercise, the three vectors are
    linearly dependent if and only if
    \begin{equation*}
      (\eta\zeta^2 - \eta^2\zeta)
      - (\xi\zeta^2 - \xi^2\zeta)
      + (\xi\eta^2 - \xi^2\eta) = 0.
    \end{equation*}
    After rearranging, we get
    \begin{align*}
      0 &= \zeta^2(\eta - \xi) - \zeta(\eta^2 - \xi^2)
          + \xi\eta(\eta - \xi) \\
        &= (\eta - \xi)(\zeta^2 - \zeta(\eta + \xi) + \xi\eta) \\
        &= (\eta - \xi)(\zeta(\zeta - \eta) - \xi(\zeta - \eta)) \\
        &= (\eta - \xi)(\zeta - \eta)(\zeta - \xi).
    \end{align*}
    So a necessary and sufficient condition for the three vectors
    $(1,\xi,\xi^2)$, $(1,\eta,\eta^2)$, and $(1,\zeta,\zeta^2)$ to be
    linearly dependent is that either $\eta = \xi$, or $\zeta = \eta$,
    or $\zeta = \xi$.
  \end{solution}
\item Guess and prove a generalization of (a) and (b) to $\C^n$.
  \begin{solution}
    The previous work suggests the following generalization: the $n$ vectors
    \begin{equation*}
      (1,\xi_1,\xi_1^2,\dots,\xi_1^{n-1}),\;
      (1,\xi_2,\xi_2^2,\dots,\xi_2^{n-1}),\;
      \dots,\;
      (1,\xi_n,\xi_n^2,\dots,\xi_n^{n-1})
    \end{equation*}
    in $\C^n$ are linearly dependent if and only if $\xi_i = \xi_j$
    for some $i\neq j$. We omit the proof.
  \end{solution}
\end{enumerate}

\Exercise7 Find two bases in $\C^4$ such that the only vectors common
to both are $(0,0,1,1)$ and $(1,1,0,0)$.
\begin{solution}
  One possible basis is
  \begin{equation*}
    \{ (1,0,0,0), \; (0,0,1,0), \; (0,0,1,1), \; (1,1,0,0) \}.
  \end{equation*}
  Then any vector $(\xi_1,\xi_2,\xi_3,\xi_4)$ in $\C^4$ can be written
  \begin{multline*}
    (\xi_1,\xi_2,\xi_3,\xi_4) \\
    = (\xi_1 - \xi_2)(1,0,0,0) + \xi_2(1,1,0,0)
    + (\xi_3 - \xi_4)(0,0,1,0) + \xi_4(0,0,1,1).
  \end{multline*}
  It is easy to show that this set of vectors is linearly independent
  (or we could make use of Theorem~2 from the upcoming section \S\ 8),
  so this is a basis.

  A different basis is
  \begin{equation*}
    \{ (1,0,1,0), \; (1,0,0,1), \; (0,0,1,1), \; (1,1,0,0) \},
  \end{equation*}
  and a similar proof will show that any vector in $\C^4$ can be
  written as a linear combination of the vectors in this set, and that
  the set is linearly independent.
\end{solution}
